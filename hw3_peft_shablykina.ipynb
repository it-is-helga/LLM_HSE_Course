{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8PEOM4NCCT9"
      },
      "source": [
        "# PEFT\n",
        "\n",
        "Сегодня LLM становятся незаменимым инструментом как для полноценного решения продуктовых задач, так и на промежуточных этапах, например, генерация разметки или создание синтетических датасетов. Обучение и дообучение таких моделей может быть ресурсозатратным, поэтому на зачастую полезно использовать Parameter-Efficient Fine-Tuning, PEFT.\n",
        "\n",
        "PEFT позволяет адаптировать крупные языковые модели под конкретные задачи, внося минимальные изменения в архитектуру и обучаясь на сравнительно небольшом объёме данных. Ключевые методы – такие как адаптеры, LoRA или DoRA – демонстрируют высокую эффективность, позволяя достичь конкурентоспособной точности при низких затратах на вычислительные мощности.\n",
        "\n",
        "Представьте, что ваша задача – определить тональность твитов. Твиты – короткие, насыщенные эмоциями и часто саркастичные сообщения, где традиционные модели могут давать сбои из-за неформального стиля и ограниченного контекста. Используя PEFT, вы можете донастроить LLM под особенности твиттер-лексикона, адаптируя модель на небольшом, но репрезентативном наборе данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3av_XVGoE0VB"
      },
      "source": [
        "## Импортируем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il6PTSX1dHJs"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet datasets bitsandbytes trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1BAjya7dXeZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, interpreter_login\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-n5o_KE-DQH"
      },
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNdKoLWk-Be-"
      },
      "outputs": [],
      "source": [
        "# Подготовим репозиторий для будущей модели и токенизатора\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw3\"  # Или как вам хочется\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4yiCPkk9_1I"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "# Этой функцией будут помечены все места, которые необходимо дозаполнить\n",
        "# Это могут быть как целые функции, так и отдельные части внутри них\n",
        "# Всегда можно воспользоваться интроспекцией и найти места использования этой функции :)\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"{DEVICE=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6g6d07CeCMo"
      },
      "source": [
        "В качестве базовой модели возьмем [`Lite-Oute-1-300M-Instruct`](https://huggingface.co/OuteAI/Lite-Oute-1-300M-Instruct).\n",
        "Она использует за основу Mistral и насчитывает около 300 млн параметров, размер контекста до 4096 токенов.\n",
        "\n",
        "Вы можете использовать любую другую модель, однако обратите внимание, что используете на `Instruct` версию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-ZP8x0-FWNp"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"OuteAI/Lite-Oute-1-300M-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlOV4EKtJYGo"
      },
      "source": [
        "## Подготовка данных [1 балл]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrsY38DxeCMp"
      },
      "source": [
        "Думаю, вы уже задумались над тем, что качество датасета для модели такого размера будет заметно влиять на перфоманс модели после обучения. Это действительно так, более того есть интересное исследование и на больших моделях: [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) показывает, что даже большие модели (например, Llama 65B) можно успешно обучить на небольшом, но исключительно качественном наборе данных.\n",
        "\n",
        "Для нашей задачи воспользуемся стандартным датасетом классификации твиттов по тональности: [`cardiffnlp/tweet_eval`](https://huggingface.co/datasets/cardiffnlp/tweet_eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhaucbodjoyJ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
        "\n",
        "IDX2NAME = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "\n",
        "def add_str_label(example):\n",
        "    example[\"str_label\"] = IDX2NAME[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "\n",
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(add_str_label)\n",
        "\n",
        "for i in range(5):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvrtTaU3JYGo"
      },
      "source": [
        "Для подготовки датасета необходими\n",
        "\n",
        "1. Задать системный промпт, в нем полезно описать задачу, а также определить формат генерации. В нашем случае это одно слово – название класса.\n",
        "2. Задать пользовательский промпт, в котором будет находиться текст на классификацию.\n",
        "3. Применить `chat_template` с помощью метода `tokenizer.apply_chat_template`, добавить начало генерации ассистента.\n",
        "4. Токенизировать датасет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwGyELHEJYGo"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = todo()\n",
        "print(f\"{SYSTEM_PROMPT=}\")\n",
        "\n",
        "\n",
        "def process_example(example, *, tokenizer, system_prompt=SYSTEM_PROMPT):\n",
        "    \"\"\"Pprocesses a single example by constructing a chat-based prompt and tokenizing it.\n",
        "\n",
        "    Process:\n",
        "        1. Constructs a conversation comprising three roles:\n",
        "            - \"system\": Provides instructions for classifying the sentiment.\n",
        "            - \"user\": Presents the input message.\n",
        "            - \"assistant\": Contains the expected sentiment answer.\n",
        "        2. Applies the chat template to generate the full prompt.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): A dictionary with keys:\n",
        "            - 'text': The message to be classified.\n",
        "            - 'str_label': The expected sentiment label.\n",
        "        system_prompt (str): System prompt to use\n",
        "\n",
        "    Returns:\n",
        "        dict: Extended dictionary containing conversational prompts:\n",
        "            - 'prompt': Input task\n",
        "            - 'full_prompt': Full conversation including task label\n",
        "    \"\"\"\n",
        "    todo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMA4x4O5joyK"
      },
      "outputs": [],
      "source": [
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(process_example, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS8wPkiXjoyK"
      },
      "outputs": [],
      "source": [
        "def tokenization(example, *, tokenizer, max_length=256):\n",
        "    \"\"\"Tokenize both prompt and full prompt and\n",
        "    save result in `input_ids` and `full_input_ids` keys along with original keys\n",
        "    \"\"\"\n",
        "    todo()\n",
        "\n",
        "\n",
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(tokenization, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iujjPbgZJYGp"
      },
      "source": [
        "Попробуем решить задачу исходной моделью!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDY0RXhlB07s"
      },
      "outputs": [],
      "source": [
        "def generate_class(model, tokenizer, input_ids):\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=16)\n",
        "    generated_text = tokenizer.decode(output_ids[0][len(input_ids[0]) :], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    input_ids = torch.tensor([dataset[\"train\"][i][\"input_ids\"]], device=DEVICE)\n",
        "    generated_text = generate_class(model, tokenizer, input_ids)\n",
        "    print(dataset[\"train\"][i][\"text\"])\n",
        "    print(dataset[\"train\"][i][\"str_label\"])\n",
        "    print(generated_text)\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0Uj4riJYGp"
      },
      "source": [
        "Если вам повезло, после слова `assistant` вы увидите определение сентиментальности входящего текста. Однако, скорее всего, модель не обучена возвращать исключительно название тональности, поэтому необходим постпроцессинг ответа. Рассмотрим самый простой способ его реализации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTxJZ3ZgJYGp"
      },
      "source": [
        "## Bonus: постпроцессинг [0.5 балла]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8zHFD3JYGp"
      },
      "source": [
        "Ниже приведена базовая реализация функции для выделения сентиментальности текста. Однако текущая реализация имеет несколько недостатков: она может не учитывать нестандартное форматирование ответа модели, дополнительные символы или ошибки пунктуации. У вас есть возможность улучшить эту функцию, обосновать выявленные ограничения и предложить более продвинутую версию, способную корректно обрабатывать различные варианты вывода модели.\n",
        "\n",
        "Если вы не хотите в этом копаться, то просто пропустите задание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwFX6rtOyx0A"
      },
      "outputs": [],
      "source": [
        "def postprocess_sentiment(output_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the sentiment classification (\"positive\" or \"negative\") from the model's output text.\n",
        "\n",
        "    Process:\n",
        "        1. Splits the output at the first occurrence of the keyword \"assistant\" and processes the text after it.\n",
        "        2. Uses a regular expression to search for the first occurrence of the words \"positive\" or \"negative\" (ignoring case).\n",
        "        3. Returns the found sentiment in lowercase. If no match is found, returns an empty string.\n",
        "\n",
        "    Parameters:\n",
        "        output_text (str): The complete text output from the model, including conversation headers.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentiment classification or empty string\n",
        "    \"\"\"\n",
        "\n",
        "    parts = output_text.split(\"assistant\", 1)\n",
        "    text_to_process = parts[1] if len(parts) > 1 else output_text\n",
        "\n",
        "    match = re.search(rf\"\\b({'|'.join(IDX2NAME.values())})\\b\", text_to_process, re.IGNORECASE)\n",
        "    return match.group(1).lower() if match else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65agkcwRJYGp"
      },
      "outputs": [],
      "source": [
        "postprocess_sentiment(\"This text is neutral, not positive or negative\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv41I7wEJYGp"
      },
      "source": [
        "## Оценка модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u90EX9_aJYGp"
      },
      "source": [
        "Давайте оценим качество нашей модели. Для этого напишем функцию `eval`, которая принимает модель и датасет, генерирует для каждого примера предсказание и вычисляет точность классификации, сравнивая полученные результаты с истинными метками. Построим матрицу ошибок и посчитаем f1 для каждого класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP0dtwuarVIX"
      },
      "outputs": [],
      "source": [
        "def pad(tensors: list[torch.Tensor], padding_value: int = 0, padding_side: str = \"left\") -> torch.Tensor:\n",
        "    \"\"\"Pads a list of tensors to the same size along their leading dimension.\n",
        "\n",
        "    Args:\n",
        "        tensors (list[torch.Tensor]): A list of tensors to be padded.\n",
        "            All tensors in the list should be of the same type and device.\n",
        "        padding_value (int, default=0): The value used to pad the tensors.\n",
        "        padding_side (str, default=\"right\"): Specifies which side of the tensor to apply padding: either 'left' or 'right'.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor containing all the padded tensors, [N; max_length]\n",
        "            where N is the number of tensors and `max_length` is the shape of the largest tensor.\n",
        "    \"\"\"\n",
        "    # ========== TODO ==========\n",
        "    #      Ваш код здесь       =\n",
        "    # ==========================\n",
        "    todo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pAZcUQoJYGp"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode\n",
        "def eval(model, dataset, tokenizer, show_conf_m=True, batch_size=100):\n",
        "    \"\"\"Evaluates the given model on the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The language model used for generating sentiment predictions.\n",
        "        dataset: An iterable collection of examples, where each example is a dict with keys:\n",
        "            - \"input_ids\": The input text message.\n",
        "            - \"str_label\": The ground truth sentiment label (e.g., \"positive\" or \"negative\").\n",
        "\n",
        "    Returns:\n",
        "        float: The macro f1 score\n",
        "    \"\"\"\n",
        "    name2idx = {v: k for k, v in IDX2NAME.items()}\n",
        "    name2idx[\"\"] = len(name2idx)\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "\n",
        "    for examples in tqdm(dataset.batch(batch_size)):\n",
        "        input_ids = pad(list(map(torch.tensor, examples[\"input_ids\"])), padding_value=tokenizer.pad_token_id).to(DEVICE)\n",
        "        attention_mask = pad(list(map(lambda it: torch.ones(len(it)), examples[\"input_ids\"])), padding_value=0).to(\n",
        "            DEVICE\n",
        "        )\n",
        "        output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=16)\n",
        "        shrinked_ids = output_ids[:, input_ids.shape[1] :]\n",
        "        texts = tokenizer.batch_decode(shrinked_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        for i in range(len(examples[\"str_label\"])):\n",
        "            predicted_sentiment = postprocess_sentiment(texts[i])\n",
        "            ground_truth.append(name2idx[examples[\"str_label\"][i]])\n",
        "            predicted.append(name2idx[predicted_sentiment])\n",
        "\n",
        "    if show_conf_m:\n",
        "        conf_m = confusion_matrix(ground_truth, predicted, labels=list(name2idx.values()))\n",
        "        disp = ConfusionMatrixDisplay(conf_m, display_labels=list(name2idx.keys()))\n",
        "        disp.plot()\n",
        "\n",
        "    f1 = f1_score(ground_truth, predicted, labels=list(name2idx.values()), average=\"macro\", zero_division=0.0)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PWl_vqvn7qL"
      },
      "outputs": [],
      "source": [
        "initial_f1 = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Initial Macro F1: {initial_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjP2PLRWtofz"
      },
      "source": [
        "# LoRA: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECs_d4pzeCMo"
      },
      "source": [
        "В традиционном fine-tuning больших языковых моделей требуется обновление огромного числа параметров, что приводит к высоким вычислительным затратам и потреблению памяти. Метод LoRA решает эту проблему, вводя низкоранговые обновления весов.\n",
        "\n",
        "Пусть $(W_0 \\in \\mathbb{R}^{d \\times k})$ - исходная матрица весов модели. При адаптации модели предполагается, что обновление весов можно аппроксимировать матрицей низкого ранга:\n",
        "$$\n",
        "\\Delta W = BA, \\quad B \\in \\mathbb{R}^{d \\times r}, \\quad A \\in \\mathbb{R}^{r \\times k},\n",
        "$$\n",
        "где $r \\ll \\min(d, k)$.\n",
        "\n",
        "Обновлённая матрица весов записывается как:\n",
        "$$\n",
        "W = W_0 + \\Delta W = W_0 + B A.\n",
        "$$\n",
        "\n",
        "Основная идея заключается в том, чтобы заморозить исходные параметры $W_0$ и обучать только матрицы $A$ и $B$. Это существенно уменьшает число обучаемых параметров, так как их суммарное количество равно $r \\times (d+k)$ по сравнению с $d \\times k$ для полной матрицы $W$.\n",
        "\n",
        "Матрицу $A$ рекомендуется инициализировать нормальным распределением $N(0, \\frac{1}{\\sqrt{r}})$, а матрицу $B$ нулями. Также не забывайте про скейлинг $\\Delta Wx$ на $\\frac{\\alpha}{r}$, где $\\alpha$ гипер-параметр. Во время обучения его можно зафиксировать $\\alpha=16$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6TIqiboJYGp"
      },
      "source": [
        "## Адаптиация модели с LoRA слоями [2 балла]\n",
        "\n",
        "В этом задачнии вам потребуется:\n",
        "1. Дописать класс `LoRALayer` который заменит слои модели\n",
        "2. Дописать функцию рекурсивного обхода модели, чтобы применить к ней `LinearWithLoRA`\n",
        "3. Обновить модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuGAjHP_tof0"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Implements a low-rank adaptation layer for a linear transformation.\n",
        "    This layer introduces a trainable low-rank update to the input tensor.\n",
        "\n",
        "    The forward computation is defined as:\n",
        "        output = alpha * (x @ A @ B)\n",
        "\n",
        "    Attributes:\n",
        "        B (nn.Parameter): A weight matrix of shape (in_dim, rank), initialized to zeros.\n",
        "        A (nn.Parameter): A weight matrix of shape (rank, out_dim), initialized with random values\n",
        "                            scaled by 1/sqrt(rank).\n",
        "        alpha (float): A scaling factor for the low-rank update.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        todo()\n",
        "\n",
        "    def forward(self, x):\n",
        "        todo()\n",
        "\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    \"\"\"Combines a standard linear layer with a LoRA (Low-Rank Adaptation) layer.\n",
        "    The forward pass returns the sum of the output of the linear layer and the low-rank update.\n",
        "\n",
        "    Attributes:\n",
        "        linear (nn.Module): The original linear layer.\n",
        "        lora (LoRALayer): The low-rank adaptation layer configured with matching input and output dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        todo()\n",
        "\n",
        "    def forward(self, x):\n",
        "        todo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p62AfaNDeFsd"
      },
      "outputs": [],
      "source": [
        "def apply_peft_to_module(model, adapter_class, r, alpha, target_submodules):\n",
        "    \"\"\"Recursively applies a parameter-efficient fine-tuning (PEFT) adapter to target submodules within a model.\n",
        "\n",
        "    This function traverses the model's children recursively. For each submodule whose name contains any\n",
        "    of the strings specified in 'target_submodules', it wraps the submodule using the provided adapter class.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to modify.\n",
        "        adapter_class (nn.Module): The adapter class (e.g., LoRALayer or LinearWithLoRA) used to wrap target submodules.\n",
        "        r (int): The rank parameter for the adapter.\n",
        "        alpha (float): The scaling factor for the low-rank update.\n",
        "        target_submodules (list of str): A list of substrings to match against submodule names for applying the adapter.\n",
        "\n",
        "    Returns:\n",
        "        None. The function updates the model in-place.\n",
        "    \"\"\"\n",
        "    todo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSM6Sf1leCMp"
      },
      "source": [
        "### Применим наш LoRA adapter к нашей модели\n",
        "\n",
        "Обычно для дешевого обучения достаточно применить LoRA к слоям для ключей `k_proj` и значений `v_proj`. Однако, если вы уверены в своих действиях, то не воспрещается обучать и другие слои с помощью LoRA :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO0GEFD9eCMp"
      },
      "outputs": [],
      "source": [
        "# Примените peft к модели\n",
        "apply_peft_to_module(model, LinearWithLoRA, r=8, alpha=16, target_submodules=[\"k_proj\", \"v_proj\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5dI0p3Ttof2"
      },
      "outputs": [],
      "source": [
        "# Заморозьте не нужные слои\n",
        "\n",
        "\n",
        "def freeze_layers(model, patterns):\n",
        "    for name, param in model.named_parameters():\n",
        "        todo()\n",
        "\n",
        "    total_params, trainable_params = 0, 0\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.requires_grad}\")\n",
        "        total_params += np.prod(param.shape)\n",
        "        if param.requires_grad:\n",
        "            trainable_params += np.prod(param.shape)\n",
        "    print(f\"Train {trainable_params}/{total_params} ({trainable_params / total_params * 100:.2f}%) parameters\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = freeze_layers(model, [\"lora\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDcZlsVuJYGq"
      },
      "source": [
        "## Обучение [1 балл]\n",
        "\n",
        "*Напутствие:*\n",
        "Пришло время приступить к обучению модели. После реализации train loop вы заслужите перерыв на 30+ минут – отличный повод с гордостью заявить, что вы заняты обучением модели. Однако длительность отдыха остаётся на ваше усмотрение. Мы рекомендуем ограничиться 2–3 эпохами файнтюна, чтобы избежать излишних вычислительных затрат и переобучения модели.\n",
        "\n",
        "Обратите внимание, что вам предстоит реализовать классический train loop на PyTorch. Здесь у вас достаточно свободы для выбора гиперпараметров (`batch_size`, `lr`, `num_epochs`). Вы можете использовать дополнительные гиперпараметры на свое усмотрение, например, для оптимизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_ZfEtmwjoyL"
      },
      "outputs": [],
      "source": [
        "def pad_collate_fn(batch: list[dict[str, torch.Tensor]], pad_token_id: int) -> dict[str, torch.Tensor]:\n",
        "    \"\"\"Collates and pads a batch of tokenized examples for model input.\n",
        "\n",
        "    This function takes a batch of examples where each example is a dictionary containing\n",
        "    token IDs for the prompt, the chosen response, and the rejected response. For each field,\n",
        "    it extracts the list of token IDs, creates a corresponding attention mask (with ones for each token),\n",
        "    and then pads the sequences using a `pad` function. The prompt sequences and their attention masks\n",
        "    are padded on the left, while the chosen and rejected sequences are padded on the right (default).\n",
        "\n",
        "    Args:\n",
        "        batch (list[dict[str, torch.Tensor]]): A list of dictionaries with input_ids keys\n",
        "        pad_token_id (int): Padding value for token IDs.\n",
        "\n",
        "    Returns:\n",
        "        dict[str, torch.Tensor]: A dictionary containing the following keys with padded tensors:\n",
        "            - \"prompt_input_ids\": Padded token IDs for the prompt (padded on the left).\n",
        "            - \"prompt_attn_mask\": Padded attention mask for the prompt (padded on the left, with 1s for actual tokens).\n",
        "            - \"chosen_input_ids\": Padded token IDs for the chosen response.\n",
        "            - \"chosen_attn_mask\": Padded attention mask for the chosen response.\n",
        "            - \"rejected_input_ids\": Padded token IDs for the rejected response.\n",
        "            - \"rejected_attn_mask\": Padded attention mask for the rejected response.\n",
        "    \"\"\"\n",
        "    # ========== TODO ==========\n",
        "    #      Ваш код здесь       =\n",
        "    # ==========================\n",
        "    todo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5EWaXx0joyL"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(\n",
        "    dataset[\"train\"],\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(pad_collate_fn, pad_token_id=tokenizer.pad_token_id),\n",
        ")\n",
        "next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUXWpHlwjoyL"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, train_dataloader, val_dataset, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "            batch = {k: (v.to(model.device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if step % 50 == 0 and step > 0:\n",
        "                avg_loss = running_loss / 50\n",
        "                print(f\"Epoch {epoch + 1}, Step {step} | Avg Loss: {avg_loss:.4f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "        val_f1 = eval(model, val_dataset, tokenizer, show_conf_m=False)\n",
        "        print(f\"Epoch {epoch + 1} | Validation F1: {val_f1}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIjehznTeOLP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(pad_collate_fn, pad_token_id=tokenizer.pad_token_id),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for n, p in model.named_parameters() if \"lora\" in n], lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdgCWxVSjoyP"
      },
      "outputs": [],
      "source": [
        "model = train_model(model, optimizer, train_dataloader, dataset[\"validation\"], NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5feQFto27yUF"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    input_ids = torch.tensor([dataset[\"train\"][i][\"input_ids\"]], device=DEVICE)\n",
        "    generated_text = generate_class(model, tokenizer, input_ids)\n",
        "    print(dataset[\"train\"][i][\"text\"])\n",
        "    print(dataset[\"train\"][i][\"str_label\"])\n",
        "    print(generated_text)\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXmr3w3eCMp"
      },
      "source": [
        "## Оценим результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B71QOPpEeCMp"
      },
      "source": [
        "Теперь увидим, как повлиял наш файнтюнинг"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEd1sn3CJYGt"
      },
      "outputs": [],
      "source": [
        "after_lora_f1 = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"After LoRA Macro F1: {after_lora_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjlSKN9tJYGt"
      },
      "outputs": [],
      "source": [
        "# Загружаем все на хаб\n",
        "\n",
        "model.push_to_hub(f\"{REPO_NAME}-lora\", private=True)\n",
        "tokenizer.push_to_hub(f\"{REPO_NAME}-lora\", private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GqXj9QeUbmv"
      },
      "outputs": [],
      "source": [
        "# Очистим память\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZLqE6lItof3"
      },
      "source": [
        "## DoRA: [Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqokyWcbeCMp"
      },
      "source": [
        "В отличие от метода LoRA, где обновление весов модели представлено в виде низкорангового произведения, метод DoRA вводит дополнительную степень гибкости за счёт применения диагональной матрицы для весового масштабирования.\n",
        "\n",
        "Пусть $W_0 \\in \\mathbb{R}^{d \\times k}$ - исходная матрица весов. В DoRA обновление весов определяется следующим образом:\n",
        "$$\n",
        "W = m \\frac{W_0 + BA}{\\|W_0 + BA \\|}\n",
        "$$\n",
        "\n",
        "Где $BA$ соответствует использованию LoRA, а $m \\in \\mathbb{R}^{k}$ обучаемый вектор."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah411RkgJYGu"
      },
      "source": [
        "## Обучение модели с помощью DoRA [4 балла]\n",
        "\n",
        "В этом задании вам потребуется:\n",
        "1. Написать с нуля класс `LinearWithDoRA` который использует написанный ранее `LoRALayer` класс\n",
        "2. Применить его к модели\n",
        "3. Обучить модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCtym5B_Gkva"
      },
      "outputs": [],
      "source": [
        "class LinearWithDoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        todo()\n",
        "\n",
        "    def forward(self, x):\n",
        "        todo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXowFAoeCMp"
      },
      "source": [
        "Сбрасываем модель и применяем наш DoRA адаптер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KBnzrTctof3"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "apply_peft_to_module(model, LinearWithDoRA, r=8, alpha=16, target_submodules=[\"k_proj\", \"v_proj\"])\n",
        "\n",
        "model = freeze_layers(model, [\"lora\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0zhC40OJYGu"
      },
      "source": [
        "### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRkx5lW9tof3"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"].take(10_000),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=partial(pad_collate_fn, pad_token_id=tokenizer.pad_token_id),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for n, p in model.named_parameters() if \"lora\" in n], lr=LEARNING_RATE, weight_decay=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyxkGpb6joyP"
      },
      "outputs": [],
      "source": [
        "model = train_model(model, optimizer, train_dataloader, dataset[\"validation\"], NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVpdAGlq_UVh"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    input_ids = torch.tensor([dataset[\"train\"][i][\"input_ids\"]], device=DEVICE)\n",
        "    generated_text = generate_class(model, tokenizer, input_ids)\n",
        "    print(dataset[\"train\"][i][\"text\"])\n",
        "    print(dataset[\"train\"][i][\"str_label\"])\n",
        "    print(generated_text)\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn4CRy9gJYGu"
      },
      "outputs": [],
      "source": [
        "after_dora_f1 = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"After DoRA Macro F1: {after_dora_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsw1xNriLOK1"
      },
      "source": [
        "Для качественного обучения доры в этой задаче нужно постараться.\n",
        "Будем считать, что если качество > 0.55, то задание с учетом правильности кода решено верно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iKx3V44Wtof4"
      },
      "outputs": [],
      "source": [
        "# Загружаем все на хаб\n",
        "\n",
        "model.push_to_hub(f\"{REPO_NAME}-dora\", private=True)\n",
        "tokenizer.push_to_hub(f\"{REPO_NAME}-dora\", private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfHn0cq2joyQ"
      },
      "outputs": [],
      "source": [
        "# Очистим память\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ2oIfqq-luU"
      },
      "source": [
        "## Frameworks way [2 балла]\n",
        "\n",
        "### QLoRA: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)\n",
        "\n",
        "Метод QLoRA направлен на эффективное дообучение предварительно обученных больших языковых моделей с использованием квантования весов. Основная идея заключается в том, что во время обратного распространения ошибки модель квантует исходные веса с точностью до 4 бит, что позволяет значительно сократить использование GPU памяти. Для обработки пиков памяти при этом применяются страничные оптимизаторы.\n",
        "\n",
        "В результате, применение QLoRA часто приводит к экономии GPU памяти примерно на $\\dfrac{1}{3}$, однако время обучения при этом может увеличиться почти на $\\dfrac{1}{4}$ по сравнению с традиционными методами дообучения.\n",
        "\n",
        "Такой компромисс между экономией памяти и увеличением времени обучения делает QLoRA привлекательным решением в сценариях, где ресурсы ограничены, а эффективность использования памяти критически важна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFawKYW2JYGu"
      },
      "source": [
        "Теперь с таким прекрасным инструментом как QLoRA можем рассмотреть более тяжелую модель аж 1.1B :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORyx9BcxTeIB"
      },
      "source": [
        "Используйте документацию библиотек [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) и [PEFT](https://huggingface.co/docs/peft/index).\n",
        "\n",
        "1. Конфигурация квантизации (`BitsAndBytesConfig`): Подберите тип 4-битной квантизации (`bnb_4bit_quant_type`) и размерность подсчёта (`bnb_4bit_compute_dtype`)\n",
        "\n",
        "2. Настройка LoRA-адаптеров (`LoraConfig`):\n",
        "    - Подберите и обоснуйте значения следующих гиперпараметров:\n",
        "        - `lora_alpha`\n",
        "        - `lora_dropout`\n",
        "        - `r`\n",
        "    - Выберите модули модели, к которым следует применять LoRA-адаптеры (`target_modules`).\n",
        "\n",
        "3. Настройка параметров обучения (`TrainingArguments`, `SFTTrainer`):\n",
        "Используя документацию и подберите параметры:\n",
        "    - `learning_rate`\n",
        "    - `num_train_epochs`\n",
        "    - `gradient_accumulation_steps`\n",
        "    - `lr_scheduler_type`\n",
        "    - `per_device_train_batch_size`\n",
        "    - другие параметры по необходимости\n",
        "\n",
        "4. Проведение обучения и анализ результатов:\n",
        "    - Запустите обучение модели.\n",
        "    - Оцените модель до и после обучения.\n",
        "    - сохраните модель\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbVVMEvgeCMq"
      },
      "outputs": [],
      "source": [
        "LARGE_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Заведем конфиг для квантизации\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=todo()\n",
        "    bnb_4bit_compute_dtype=todo()\n",
        ")\n",
        "\n",
        "# Инициализация квантованной модели\n",
        "model = AutoModelForCausalLM.from_pretrained(LARGE_MODEL_NAME, quantization_config=bnb_config)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3eNbrPleCMr"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LARGE_MODEL_NAME)\n",
        "tokenizer.pad_token = \"<PAD>\"\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaC7cr07KR6w"
      },
      "outputs": [],
      "source": [
        "for split, data in dataset.items():\n",
        "    dataset[split] = data.map(process_example, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "    dataset[split] = data.map(tokenization, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RoZzzaJjoyQ"
      },
      "outputs": [],
      "source": [
        "initial_f1_large_model = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Initial F1 large model: {initial_f1_large_model:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPvkzXT4eCMr"
      },
      "source": [
        "### Обучим QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAtG5fqeCMr"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=todo(),\n",
        "    lora_dropout=todo(),\n",
        "    r=todo(),\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=todo(),\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnekx15FOQf7"
      },
      "source": [
        "`SFTTrainer` supports conversational format:\n",
        "```\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVpClyiJObAE"
      },
      "outputs": [],
      "source": [
        "def convert_instruction_format(example, system_prompt=SYSTEM_PROMPT):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Text: {example['text']}\"},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"str_label\"]},\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "\n",
        "sft_dataset = dataset.copy()\n",
        "for split, data in dataset.items():\n",
        "    sft_dataset[split] = data.map(\n",
        "        convert_instruction_format,\n",
        "        remove_columns=[\"text\", \"label\", \"str_label\", \"prompt\", \"full_prompt\", \"input_ids\", \"full_input_ids\"],\n",
        "    )\n",
        "\n",
        "for i in range(2):\n",
        "    for k, v in sft_dataset[\"train\"][i].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"=\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_qexatpeCMr"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(todo())\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=sft_dataset[\"train\"],\n",
        "    args=training_arguments,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZD2lveJYGv"
      },
      "outputs": [],
      "source": [
        "qlora_large_model = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"QLoRA F1 large model: {qlora_large_model:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV8cJJTyJYGv"
      },
      "outputs": [],
      "source": [
        "# Загружаем все на хаб\n",
        "\n",
        "model.push_to_hub(f\"{REPO_NAME}-tinyllama-qlora\", private=True)\n",
        "tokenizer.push_to_hub(f\"{REPO_NAME}-tinyllamma-qlora\", private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дополнительные баллы\n",
        "\n",
        "Вы также можно заработать дополнительные баллы:\n",
        "- Оформить репозитории на 🤗 (можно сделать коллекцию, так как у нас 3 репозитория): карточка модели с описанием задания, репортом качества и примерами генерации **[1 балл]**"
      ],
      "metadata": {
        "id": "RzNfBG6cfcwC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LVxsIyFJdUC"
      },
      "source": [
        "# Специальный раздел для проверяющего"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE6NoTX3Jxp4"
      },
      "source": [
        "## LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIMzSBypJgMF"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(f\"{REPO_NAME}-lora\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{REPO_NAME}-lora\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "LoRA_saved_model_accuracy = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Accuracy after LoRA training: {LoRA_saved_model_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zG4EGiMJ0Um"
      },
      "source": [
        "## DoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRq41oNiJ1fi"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(f\"{REPO_NAME}-dora\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{REPO_NAME}-dora\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "DoRA_saved_model_accuracy = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Accuracy after DoRA training: {DoRA_saved_model_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z-4DEA6J19f"
      },
      "source": [
        "## QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU6wcevzJ3Gn"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(f\"{REPO_NAME}-tinyllama-qlora\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{REPO_NAME}-tinyllama-qlora\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "QLoRA_saved_model_accuracy = eval(model, dataset[\"test\"], tokenizer)\n",
        "print(f\"Accuracy after tinyllama QLoRA training: {QLoRA_saved_model_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "6LVxsIyFJdUC"
      ]
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "py_3_10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}